{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashnavani/Akashnavani/blob/main/datavilab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a PySpark program to create a DataFrame with four columns: “name”, “age”, “city”, and\n",
        "# “gender” and perform the following operations:\n",
        "# Insert minimum 10 values for the given columns.\n",
        "#  Filter rows with age greater than 30.\n",
        "#  Add a new column named it “tax”.\n",
        "#  Rename the “age” column to “years”.\n",
        "#  Drop Multiple Columns from the given data frame.\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
        "# Creates a Spark application named \"PySparkExample\"\n",
        "# If a Spark session already exists, it reuses it (getOrCreate()).\n",
        "\n",
        "data = [\n",
        "    (\"Alice\", 25, \"New York\", \"F\"),\n",
        "    (\"Bob\", 35, \"Los Angeles\", \"M\"),\n",
        "    (\"Charlie\", 32, \"Chicago\", \"M\"),\n",
        "    (\"David\", 28, \"Houston\", \"M\"),\n",
        "    (\"Eva\", 45, \"Phoenix\", \"F\"),\n",
        "    (\"Frank\", 22, \"Philadelphia\", \"M\"),\n",
        "    (\"Grace\", 30, \"San Antonio\", \"F\"),\n",
        "    (\"Helen\", 29, \"San Diego\", \"F\"),\n",
        "    (\"Ian\", 41, \"Dallas\", \"M\"),\n",
        "    (\"Jane\", 33, \"San Jose\", \"F\")\n",
        "]\n",
        "\n",
        "columns = [\"name\", \"age\", \"city\", \"gender\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show() # I am displaying the data frame\n",
        "filtered_df = df.filter(df.age > 30)\n",
        "print(\"displaying more than age 30\")\n",
        "filtered_df.show()\n",
        "\n",
        "\n",
        "\n",
        "df_with_tax = filtered_df.withColumn(\"tax\",lit(0.1))\n",
        "print(\"dataframe with new column tax\")\n",
        "df_with_tax.show()\n",
        "\n",
        "\n",
        "df_renamed = df_with_tax.withColumnRenamed(\"age\",\"years\")\n",
        "print(\"dataframe with column renamed\")\n",
        "df_renamed.show()\n",
        "\n",
        "final_df = df_renamed.drop(\"city\",\"gender\")\n",
        "print(\"After dropping\")\n",
        "final_df.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFXDdXkueUsF",
        "outputId": "55783a7c-ef4f-4d95-e1b3-d342ce744558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-------+---+------------+------+\n",
            "|   name|age|        city|gender|\n",
            "+-------+---+------------+------+\n",
            "|  Alice| 25|    New York|     F|\n",
            "|    Bob| 35| Los Angeles|     M|\n",
            "|Charlie| 32|     Chicago|     M|\n",
            "|  David| 28|     Houston|     M|\n",
            "|    Eva| 45|     Phoenix|     F|\n",
            "|  Frank| 22|Philadelphia|     M|\n",
            "|  Grace| 30| San Antonio|     F|\n",
            "|  Helen| 29|   San Diego|     F|\n",
            "|    Ian| 41|      Dallas|     M|\n",
            "|   Jane| 33|    San Jose|     F|\n",
            "+-------+---+------------+------+\n",
            "\n",
            "displaying more than age 30\n",
            "+-------+---+-----------+------+\n",
            "|   name|age|       city|gender|\n",
            "+-------+---+-----------+------+\n",
            "|    Bob| 35|Los Angeles|     M|\n",
            "|Charlie| 32|    Chicago|     M|\n",
            "|    Eva| 45|    Phoenix|     F|\n",
            "|    Ian| 41|     Dallas|     M|\n",
            "|   Jane| 33|   San Jose|     F|\n",
            "+-------+---+-----------+------+\n",
            "\n",
            "dataframe with new column tax\n",
            "+-------+---+-----------+------+---+\n",
            "|   name|age|       city|gender|tax|\n",
            "+-------+---+-----------+------+---+\n",
            "|    Bob| 35|Los Angeles|     M|0.1|\n",
            "|Charlie| 32|    Chicago|     M|0.1|\n",
            "|    Eva| 45|    Phoenix|     F|0.1|\n",
            "|    Ian| 41|     Dallas|     M|0.1|\n",
            "|   Jane| 33|   San Jose|     F|0.1|\n",
            "+-------+---+-----------+------+---+\n",
            "\n",
            "dataframe with column renamed\n",
            "+-------+-----+-----------+------+---+\n",
            "|   name|years|       city|gender|tax|\n",
            "+-------+-----+-----------+------+---+\n",
            "|    Bob|   35|Los Angeles|     M|0.1|\n",
            "|Charlie|   32|    Chicago|     M|0.1|\n",
            "|    Eva|   45|    Phoenix|     F|0.1|\n",
            "|    Ian|   41|     Dallas|     M|0.1|\n",
            "|   Jane|   33|   San Jose|     F|0.1|\n",
            "+-------+-----+-----------+------+---+\n",
            "\n",
            "After dropping\n",
            "+-------+-----+---+\n",
            "|   name|years|tax|\n",
            "+-------+-----+---+\n",
            "|    Bob|   35|0.1|\n",
            "|Charlie|   32|0.1|\n",
            "|    Eva|   45|0.1|\n",
            "|    Ian|   41|0.1|\n",
            "|   Jane|   33|0.1|\n",
            "+-------+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a PySpark program to create a DataFrame containing information about various products,\n",
        "# including ProductID, ProductName, Category, Price, StockQuantity, & Rating and perform the\n",
        "# following operations:\n",
        "#\n",
        "# Insert minimum 10 values for the given columns.\n",
        "# Sort the DataFrame first by Price in descending order and then by Category in ascending\n",
        "# order.\n",
        "# Find the total sales amount for each product by category.\n",
        "# Find the total sales amount and the total quantity sold for each product.\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,sum as _sum,round\n",
        "#Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ProductDataFrame\").getOrCreate()\n",
        "#Creates a Spark application named \"PySparkExample\"\n",
        "#If a Spark session already exists, it reuses it (getOrCreate()).\n",
        "\n",
        "data = [\n",
        "    (1, \"Laptop\", \"Electronics\", 55000, 25, 4.5),\n",
        "    (2, \"Smartphone\", \"Electronics\", 22000, 50, 4.3),\n",
        "    (3, \"Headphones\", \"Accessories\", 2500, 120, 4.1),\n",
        "    (4, \"Office Chair\", \"Furniture\", 8500, 15, 4.6),\n",
        "    (5, \"Water Bottle\", \"Home & Kitchen\", 500, 200, 4.2),\n",
        "    (6, \"Microwave Oven\", \"Appliances\", 12000, 18, 4.4),\n",
        "    (7, \"Running Shoes\", \"Footwear\", 3500, 40, 4.0),\n",
        "    (8, \"Backpack\", \"Bags\", 1800, 75, 4.3),\n",
        "    (9, \"Wrist Watch\", \"Accessories\", 6000, 30, 4.5),\n",
        "    (10, \"Study Table\", \"Furniture\", 9500, 10, 4.7)\n",
        "]\n",
        "\n",
        "columns = [\"ProductID\", \"ProductName\", \"Category\", \"Price\", \"StockQuantity\", \"Rating\"]\n",
        "\n",
        "\n",
        "#Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show() #I am displaying the data frame\n",
        "\n",
        "print(\"Sorted dataframe (Price desc ,Category asc):\")\n",
        "sorted_df = df.orderBy(col(\"Price\").desc(),col(\"Category\").asc())\n",
        "#Or sorted_df = df.orderBy(col(\"Category\").asc(),col(\"Price\").desc())\n",
        "\n",
        "sorted_df.show()\n",
        "\n",
        "df_with_sales = df.withColumn(\"SalesAmount\",col(\"Price\")*col(\"StockQuantity\"))\n",
        "print(\"Total sales amount by Category\")\n",
        "sales_by_category = df_with_sales.groupBy(\"Category\").agg(round(_sum(\"SalesAmount\"),2).alias(\"TotalSalesAmount\"))\n",
        "sales_by_category.show()\n",
        "\n",
        "print(\"Total sales Amount and Quantity by product:\")\n",
        "sales_by_product = df_with_sales.groupBy(\"ProductName\").agg(round(_sum(\"SalesAmount\"),2).alias(\"TotalSalesAmount\"),_sum(\"StockQuantity\").alias(\"TotalQuantitySold\"))\n",
        "sales_by_product.show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAnCQQfzkYNw",
        "outputId": "a621a9ff-6a9a-435d-d2ee-1c68412c1960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "|ProductID|   ProductName|      Category|Price|StockQuantity|Rating|\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "|        1|        Laptop|   Electronics|55000|           25|   4.5|\n",
            "|        2|    Smartphone|   Electronics|22000|           50|   4.3|\n",
            "|        3|    Headphones|   Accessories| 2500|          120|   4.1|\n",
            "|        4|  Office Chair|     Furniture| 8500|           15|   4.6|\n",
            "|        5|  Water Bottle|Home & Kitchen|  500|          200|   4.2|\n",
            "|        6|Microwave Oven|    Appliances|12000|           18|   4.4|\n",
            "|        7| Running Shoes|      Footwear| 3500|           40|   4.0|\n",
            "|        8|      Backpack|          Bags| 1800|           75|   4.3|\n",
            "|        9|   Wrist Watch|   Accessories| 6000|           30|   4.5|\n",
            "|       10|   Study Table|     Furniture| 9500|           10|   4.7|\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "\n",
            "Sorted dataframe (Price desc ,Category asc):\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "|ProductID|   ProductName|      Category|Price|StockQuantity|Rating|\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "|        1|        Laptop|   Electronics|55000|           25|   4.5|\n",
            "|        2|    Smartphone|   Electronics|22000|           50|   4.3|\n",
            "|        6|Microwave Oven|    Appliances|12000|           18|   4.4|\n",
            "|       10|   Study Table|     Furniture| 9500|           10|   4.7|\n",
            "|        4|  Office Chair|     Furniture| 8500|           15|   4.6|\n",
            "|        9|   Wrist Watch|   Accessories| 6000|           30|   4.5|\n",
            "|        7| Running Shoes|      Footwear| 3500|           40|   4.0|\n",
            "|        3|    Headphones|   Accessories| 2500|          120|   4.1|\n",
            "|        8|      Backpack|          Bags| 1800|           75|   4.3|\n",
            "|        5|  Water Bottle|Home & Kitchen|  500|          200|   4.2|\n",
            "+---------+--------------+--------------+-----+-------------+------+\n",
            "\n",
            "Total sales amount by Category\n",
            "+--------------+----------------+\n",
            "|      Category|TotalSalesAmount|\n",
            "+--------------+----------------+\n",
            "|Home & Kitchen|          100000|\n",
            "|   Electronics|         2475000|\n",
            "|   Accessories|          480000|\n",
            "|     Furniture|          222500|\n",
            "|          Bags|          135000|\n",
            "|      Footwear|          140000|\n",
            "|    Appliances|          216000|\n",
            "+--------------+----------------+\n",
            "\n",
            "Total sales Amount and Quantity by product:\n",
            "+--------------+----------------+-----------------+\n",
            "|   ProductName|TotalSalesAmount|TotalQuantitySold|\n",
            "+--------------+----------------+-----------------+\n",
            "|  Office Chair|          127500|               15|\n",
            "|  Water Bottle|          100000|              200|\n",
            "|        Laptop|         1375000|               25|\n",
            "|    Smartphone|         1100000|               50|\n",
            "|    Headphones|          300000|              120|\n",
            "|Microwave Oven|          216000|               18|\n",
            "|      Backpack|          135000|               75|\n",
            "| Running Shoes|          140000|               40|\n",
            "|   Study Table|           95000|               10|\n",
            "|   Wrist Watch|          180000|               30|\n",
            "+--------------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#not in syllabus\n",
        "# Write a PySpark program to create a DataFrame containing information about employees in a company. including the following columns:\n",
        "#EmployeeID, EmployeeName, Department, Salary, Experience, Age\n",
        "#Perform the following operations:\n",
        "#Insert at least 10 sample records for the given columns.\n",
        "#Sort the DataFrame first by Salary in ascending order and then by Experience in descending order.\n",
        "#Find the average salary for each department.\n",
        "#Find the total years of experience and average age for each department\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, sum as _sum\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"EmployeeData\").getOrCreate()\n",
        "\n",
        "# Sample data: list of tuples (EmployeeID, EmployeeName, Department, Salary, Experience, Age)\n",
        "data = [\n",
        "    (1, \"Alice\", \"HR\", 60000, 5, 30),\n",
        "    (2, \"Bob\", \"IT\", 75000, 7, 35),\n",
        "    (3, \"Charlie\", \"Finance\", 80000, 6, 40),\n",
        "    (4, \"David\", \"IT\", 70000, 8, 28),\n",
        "    (5, \"Eva\", \"HR\", 62000, 4, 32),\n",
        "    (6, \"Frank\", \"Finance\", 85000, 9, 45),\n",
        "    (7, \"Grace\", \"IT\", 72000, 3, 26),\n",
        "    (8, \"Hannah\", \"HR\", 58000, 6, 29),\n",
        "    (9, \"Ian\", \"Finance\", 79000, 5, 38),\n",
        "    (10, \"Jane\", \"IT\", 68000, 7, 34),\n",
        "]\n",
        "\n",
        "#Define schema columns\n",
        "columns = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"Salary\", \"Experience\", \"Age\"]\n",
        "\n",
        "#Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "#Sort DataFrame by Salary ascending, then Experience descending\n",
        "sorted_df = df.orderBy(col(\"Salary\").asc(), col(\"Experience\").desc())\n",
        "print(\"Sorted DataFrame (Salary ascending, Experience descending):\")\n",
        "sorted_df.show()\n",
        "\n",
        "#Average salary per department\n",
        "avg_salary = df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\"))\n",
        "print(\"Average Salary per Department:\")\n",
        "avg_salary.show()\n",
        "\n",
        "#Total experience and average age per department\n",
        "dept_stats = df.groupBy(\"Department\").agg(\n",
        "    _sum(\"Experience\").alias(\"TotalExperience\"),\n",
        "    avg(\"Age\").alias(\"AvgAge\"))\n",
        "print(\"Total Experience and Average Age per Department:\")\n",
        "dept_stats.show()\n",
        "\n",
        "#Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2QYqjrMoUYS",
        "outputId": "11e5f841-bf26-4c11-8571-fdb402fda7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+----------+------------+----------+------+----------+---+\n",
            "|EmployeeID|EmployeeName|Department|Salary|Experience|Age|\n",
            "+----------+------------+----------+------+----------+---+\n",
            "|         1|       Alice|        HR| 60000|         5| 30|\n",
            "|         2|         Bob|        IT| 75000|         7| 35|\n",
            "|         3|     Charlie|   Finance| 80000|         6| 40|\n",
            "|         4|       David|        IT| 70000|         8| 28|\n",
            "|         5|         Eva|        HR| 62000|         4| 32|\n",
            "|         6|       Frank|   Finance| 85000|         9| 45|\n",
            "|         7|       Grace|        IT| 72000|         3| 26|\n",
            "|         8|      Hannah|        HR| 58000|         6| 29|\n",
            "|         9|         Ian|   Finance| 79000|         5| 38|\n",
            "|        10|        Jane|        IT| 68000|         7| 34|\n",
            "+----------+------------+----------+------+----------+---+\n",
            "\n",
            "Sorted DataFrame (Salary ascending, Experience descending):\n",
            "+----------+------------+----------+------+----------+---+\n",
            "|EmployeeID|EmployeeName|Department|Salary|Experience|Age|\n",
            "+----------+------------+----------+------+----------+---+\n",
            "|         8|      Hannah|        HR| 58000|         6| 29|\n",
            "|         1|       Alice|        HR| 60000|         5| 30|\n",
            "|         5|         Eva|        HR| 62000|         4| 32|\n",
            "|        10|        Jane|        IT| 68000|         7| 34|\n",
            "|         4|       David|        IT| 70000|         8| 28|\n",
            "|         7|       Grace|        IT| 72000|         3| 26|\n",
            "|         2|         Bob|        IT| 75000|         7| 35|\n",
            "|         9|         Ian|   Finance| 79000|         5| 38|\n",
            "|         3|     Charlie|   Finance| 80000|         6| 40|\n",
            "|         6|       Frank|   Finance| 85000|         9| 45|\n",
            "+----------+------------+----------+------+----------+---+\n",
            "\n",
            "Average Salary per Department:\n",
            "+----------+-----------------+\n",
            "|Department|        AvgSalary|\n",
            "+----------+-----------------+\n",
            "|        HR|          60000.0|\n",
            "|   Finance|81333.33333333333|\n",
            "|        IT|          71250.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "Total Experience and Average Age per Department:\n",
            "+----------+---------------+------------------+\n",
            "|Department|TotalExperience|            AvgAge|\n",
            "+----------+---------------+------------------+\n",
            "|        HR|             15|30.333333333333332|\n",
            "|   Finance|             20|              41.0|\n",
            "|        IT|             25|             30.75|\n",
            "+----------+---------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using PySpark, analyze airline flight data (e.g., departure and arrival times, delays, carrier information) and perform the following operations:    Load a CSV file containing airline flight data Filter flights that were more than 15 minutes delayed. Analyze whether there is any correlation between the flight length and the likelihood of a delay. Dataset:https://drive.google.com/drive/folders/1KTpKBf5w8VOyPNTTIniM9oN- pwaIgjqf?usp=drive_link\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, corr\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FlightDataAnalysis\").getOrCreate()\n",
        "\n",
        "file_path = \"/content/flights.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "#to check\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "#Filter flights >15 minutes delayed\n",
        "delayed_flights = df.filter(col(\"arr_delay\") > 15)\n",
        "print(f\"Total flights with arrival delay >15 mins is : {delayed_flights.count()} \")\n",
        "delayed_flights.show(5)\n",
        "\n",
        "correlation_value = df.corr(\"distance\",\"arr_delay\")\n",
        "print(f\"correlation of flight distance and arrival delay is:{correlation_value} \")\n",
        "\n",
        "df_with_binary_delay = df.withColumn(\"IS_DELAYED\",(col(\"arr_delay\")>15).cast(\"integer\"))\n",
        "df_with_binary_delay.show(5)\n",
        "\n",
        "correlation_binary = df_with_binary_delay.corr(\"distance\",\"IS_DELAYED\")\n",
        "print(f\"correlation of flight distance and likelyhood of delay is:{correlation_binary} \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i44TukTaDEc",
        "outputId": "155bc515-e309-422d-d469-59b5cb1f830b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- dep_time: double (nullable = true)\n",
            " |-- sched_dep_time: integer (nullable = true)\n",
            " |-- dep_delay: double (nullable = true)\n",
            " |-- arr_time: double (nullable = true)\n",
            " |-- sched_arr_time: integer (nullable = true)\n",
            " |-- arr_delay: double (nullable = true)\n",
            " |-- carrier: string (nullable = true)\n",
            " |-- flight: integer (nullable = true)\n",
            " |-- tailnum: string (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- dest: string (nullable = true)\n",
            " |-- air_time: double (nullable = true)\n",
            " |-- distance: integer (nullable = true)\n",
            " |-- hour: integer (nullable = true)\n",
            " |-- minute: integer (nullable = true)\n",
            " |-- time_hour: timestamp (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|\n",
            "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|\n",
            "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total flights with arrival delay >15 mins is : 5238 \n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|\n",
            "|  6|2013|    1|  1|   555.0|           600|     -5.0|   913.0|           854|     19.0|     B6|   507| N516JB|   EWR| FLL|   158.0|    1065|   6|     0|2013-01-01 06:00:00|     JetBlue Airways|\n",
            "| 14|2013|    1|  1|   559.0|           600|     -1.0|   941.0|           910|     31.0|     AA|   707| N3DUAA|   LGA| DFW|   257.0|    1389|   6|     0|2013-01-01 06:00:00|American Airlines...|\n",
            "| 21|2013|    1|  1|   602.0|           605|     -3.0|   821.0|           805|     16.0|     MQ|  4401| N730MQ|   LGA| DTW|   105.0|     502|   6|     5|2013-01-01 06:00:00|           Envoy Air|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "correlation of flight distance and arrival delay is:-0.07196468279825374 \n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+----------+\n",
            "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|IS_DELAYED|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+----------+\n",
            "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|         0|\n",
            "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|         1|\n",
            "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|         1|\n",
            "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|         0|\n",
            "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|         0|\n",
            "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "correlation of flight distance and likelyhood of delay is:-0.029647362129586415 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Consider airline flight data, given in the previous question. Perform the following operation\n",
        "# using PySpark\n",
        "# Group the data by airline carrier and compute the average delay for each one.\n",
        "# Determine the top five routes (origin-destination) with the highest average delay.\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, corr\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FlightDataAnalysis\").getOrCreate()\n",
        "\n",
        "file_path = \"/content/flights.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "print(\"Average delay of each group \")\n",
        "avg_delay_by_carrier = df.groupBy(\"carrier\").agg(F.avg(\"arr_delay\").alias(\"avg_delay\"))\n",
        "avg_delay_by_carrier = avg_delay_by_carrier.orderBy(F.desc(\"avg_delay\"))\n",
        "avg_delay_by_carrier.show()\n",
        "\n",
        "print(\"Top 5 routes ordered by average delay\")\n",
        "df = df.withColumn(\"route\",F.concat_ws(\"-\",F.col(\"origin\"),F.col(\"dest\")))\n",
        "\n",
        "avg_delay_by_route = df.groupBy(\"route\").agg(F.avg(\"arr_delay\").alias(\"avg_delay\"))\n",
        "top5_routes = avg_delay_by_route.orderBy(F.desc(\"avg_delay\")).limit(5)\n",
        "top5_routes.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15ZxkyV6jWoN",
        "outputId": "f89ceba4-c159-4f35-ac2b-a9a15954a7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average delay of each group \n",
            "+-------+--------------------+\n",
            "|carrier|           avg_delay|\n",
            "+-------+--------------------+\n",
            "|     F9|  19.791808873720136|\n",
            "|     OO|                17.0|\n",
            "|     EV|  16.401793298725813|\n",
            "|     FL|  12.094907407407407|\n",
            "|     YV|   8.615062761506277|\n",
            "|     WN|   7.866318436834978|\n",
            "|     MQ|   7.437600529851452|\n",
            "|     B6|  6.4721437855766215|\n",
            "|     9E|   5.823355601233299|\n",
            "|     UA|  2.6395006428601056|\n",
            "|     AA|  0.4358667165855485|\n",
            "|     US|-0.01197532357566227|\n",
            "|     DL|  -2.464905544463216|\n",
            "|     HA|  -4.268115942028985|\n",
            "|     VX|  -4.894525364138624|\n",
            "|     AS|  -6.952218430034129|\n",
            "+-------+--------------------+\n",
            "\n",
            "Top 5 routes ordered by average delay\n",
            "+-------+------------------+\n",
            "|  route|         avg_delay|\n",
            "+-------+------------------+\n",
            "|LGA-SBN|40.666666666666664|\n",
            "|EWR-CAE| 39.51219512195122|\n",
            "|EWR-TUL| 37.61290322580645|\n",
            "|EWR-JAC|              35.8|\n",
            "|EWR-OKC|        34.6484375|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a Movie dataset containing user ratings for movies, using PySpark SQL perform the\n",
        "# following operations\n",
        "\n",
        "# Load a CSV file containing movie data\n",
        "# Create temporary views for movies and ratings.\n",
        "# Write queries to find the top 10 highest-rated movies with at least 10 ratings.\n",
        "# Dataset:https://drive.google.com/file/d/17PFBafCd0J8brMNjdVV-NyNCBI01-7fr/view?usp=drive_link\n",
        "# Find the most active users (users who have rated the most movies).\n",
        "from  pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"MovieRatingsAnalysis\").getOrCreate()\n",
        "# 1. Load the CSV files\n",
        "\n",
        "#Adjust file paths as needed\n",
        "movies_df = spark.read.option(\"header\", True).csv(\"/content/movies.csv\", inferSchema=True)\n",
        "ratings_df = spark.read.option(\"header\", True).csv(\"/content/ratings.csv\", inferSchema=True)\n",
        "\n",
        "#2. Create temporary views\n",
        "movies_df.createOrReplaceTempView(\"movies\")\n",
        "ratings_df.createOrReplaceTempView(\"ratings\")\n",
        "\n",
        "#Let me display the views\n",
        "print(\"***** ***MOVIES AND RATINGS VIENS *********\")\n",
        "spark.sql(\"SELECT * FROM movies LIMIT 5\").show()\n",
        "spark.sql(\"SELECT * FROM ratings LIMIT 5\").show()\n",
        "\n",
        "#can i create a view with only few columns\n",
        "movies_selected = movies_df.select(\"movieId\", \"title\")\n",
        "movies_selected.createOrReplaceTempView(\"movies_view\")\n",
        "print(\"********....................MOVIES VIEW.....\")\n",
        "spark.sql(\"SELECT * FROM movies_view LIMIT 5\").show()\n",
        "\n",
        "\n",
        "\n",
        "top_movies_query = \"\"\" SELECT m.title ,  COUNT(r.rating) AS num_ratings,ROUND(AVG(r.rating),2) AS avg_rating\n",
        "FROM ratings r JOIN movies m ON r.movieId = m.movieId GROUP BY m.title HAVING COUNT(r.rating) >= 10 ORDER BY avg_rating DESC ,num_ratings DESC LIMIT 10 \"\"\"\n",
        "top_movies = spark.sql(top_movies_query)\n",
        "print(\"==== Top 10 Highest RAted Movies (with at least 10 ratings) =====\")\n",
        "top_movies.show(10,truncate=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "active_users_query = \"\"\" SELECT userId , COUNT(movieId) AS total_ratings FROM ratings GROUP BY userId ORDER BY total_ratings DESC LIMIT 10 \"\"\"\n",
        "active_users = spark.sql(active_users_query)\n",
        "print(\"==== Most Active users (user who rated the most movies) =====\")\n",
        "active_users.show(10,truncate=False) #without trunctate title may be not fully displayed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "top_movies.write.mode(\"overwrite\").csv(\"output/top_movies.csv\",header=True)\n",
        "active_users.write.mode(\"overwrite\").csv(\"output/active_users.csv\",header=True)\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qzc1KekufGv",
        "outputId": "72038b42-9f35-4013-b926-25561881ae4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** ***MOVIES AND RATINGS VIENS *********\n",
            "+-------+--------------------+--------------------+\n",
            "|movieId|               title|              genres|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
            "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
            "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
            "|      5|Father of the Bri...|              Comedy|\n",
            "+-------+--------------------+--------------------+\n",
            "\n",
            "+------+-------+------+---------+\n",
            "|userId|movieId|rating|timestamp|\n",
            "+------+-------+------+---------+\n",
            "|     1|      1|   4.0|964982703|\n",
            "|     1|      3|   4.0|964981247|\n",
            "|     1|      6|   4.0|964982224|\n",
            "|     1|     47|   5.0|964983815|\n",
            "|     1|     50|   5.0|964982931|\n",
            "+------+-------+------+---------+\n",
            "\n",
            "********....................MOVIES VIEW.....\n",
            "+-------+--------------------+\n",
            "|movieId|               title|\n",
            "+-------+--------------------+\n",
            "|      1|    Toy Story (1995)|\n",
            "|      2|      Jumanji (1995)|\n",
            "|      3|Grumpier Old Men ...|\n",
            "|      4|Waiting to Exhale...|\n",
            "|      5|Father of the Bri...|\n",
            "+-------+--------------------+\n",
            "\n",
            "==== Top 10 Highest RAted Movies (with at leqast 10 ratings) =====\n",
            "+---------------------------------------------+-----------+----------+\n",
            "|title                                        |num_ratings|avg_rating|\n",
            "+---------------------------------------------+-----------+----------+\n",
            "|Secrets & Lies (1996)                        |11         |4.59      |\n",
            "|Guess Who's Coming to Dinner (1967)          |11         |4.55      |\n",
            "|Paths of Glory (1957)                        |12         |4.54      |\n",
            "|Streetcar Named Desire, A (1951)             |20         |4.48      |\n",
            "|Celebration, The (Festen) (1998)             |12         |4.46      |\n",
            "|Shawshank Redemption, The (1994)             |317        |4.43      |\n",
            "|Ran (1985)                                   |15         |4.43      |\n",
            "|His Girl Friday (1940)                       |14         |4.39      |\n",
            "|All Quiet on the Western Front (1930)        |10         |4.35      |\n",
            "|Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)|27         |4.33      |\n",
            "+---------------------------------------------+-----------+----------+\n",
            "\n",
            "==== Most Active users (user who rated the most movies) =====\n",
            "+------+-------------+\n",
            "|userId|total_ratings|\n",
            "+------+-------------+\n",
            "|414   |2698         |\n",
            "|599   |2478         |\n",
            "|474   |2108         |\n",
            "|448   |1864         |\n",
            "|274   |1346         |\n",
            "|610   |1302         |\n",
            "|68    |1260         |\n",
            "|380   |1218         |\n",
            "|606   |1115         |\n",
            "|288   |1055         |\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#out of syllabus\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum as _sum, avg\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"StudentRecordsAnalysis\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (301, \"Alice\", 10, 85, 78, 88),\n",
        "    (302, \"Bob\", 10, 92, 85, 79),\n",
        "    (303, \"Charlie\", 9, 70, 75, 80),\n",
        "    (304, \"David\", 9, 88, 82, 85),\n",
        "    (305, \"Eva\", 10, 95, 89, 92),\n",
        "    (306, \"Frank\", 9, 60, 65, 70),\n",
        "    (307, \"Grace\", 10, 78, 80, 76),\n",
        "    (308, \"Henry\", 9, 82, 79, 84),\n",
        "    (309, \"Irene\", 10, 90, 88, 91),\n",
        "    (310, \"John\", 9, 68, 72, 75)\n",
        "]\n",
        "\n",
        "# Define schema columns\n",
        "columns = [\"StudentID\", \"Name\", \"Class\", \"MathScore\", \"ScienceScore\", \"EnglishScore\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# 3. Filter students who scored more than 80 in Math\n",
        "math_above_80_df = df.filter(col(\"MathScore\") > 80)\n",
        "\n",
        "# 4. Add a new column TotalScore = Math + Science + English\n",
        "df = df.withColumn(\"TotalScore\", col(\"MathScore\") + col(\"ScienceScore\") + col(\"EnglishScore\"))\n",
        "\n",
        "# 5. Rename column 'Class' to 'Grade'\n",
        "df = df.withColumnRenamed(\"Class\", \"Grade\")\n",
        "\n",
        "# 6. Drop the EnglishScore column\n",
        "df = df.drop(\"EnglishScore\")\n",
        "\n",
        "# 7. Compute average score in each subject for all students\n",
        "avg_scores = df.select(\n",
        "    avg(\"MathScore\").alias(\"AvgMathScore\"),\n",
        "    avg(\"ScienceScore\").alias(\"AvgScienceScore\"),\n",
        "    avg(\"TotalScore\").alias(\"AvgTotalScore\")  # Optional, since English dropped, TotalScore is still sum\n",
        ").collect()[0]\n",
        "\n",
        "# Show results\n",
        "print(\"Students with MathScore > 80:\")\n",
        "math_above_80_df.show()\n",
        "\n",
        "print(\"DataFrame after adding TotalScore, renaming Class to Grade, and dropping EnglishScore:\")\n",
        "df.show()\n",
        "\n",
        "print(\"Average Scores:\")\n",
        "print(f\"Average Math Score: {avg_scores['AvgMathScore']:.2f}\")\n",
        "print(f\"Average Science Score: {avg_scores['AvgScienceScore']:.2f}\")\n",
        "print(f\"Average Total Score: {avg_scores['AvgTotalScore']:.2f}\")\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5XRx8ujizwv",
        "outputId": "79ac096a-9127-4b82-df0e-febb0586e8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students with MathScore > 80:\n",
            "+---------+-----+-----+---------+------------+------------+\n",
            "|StudentID| Name|Class|MathScore|ScienceScore|EnglishScore|\n",
            "+---------+-----+-----+---------+------------+------------+\n",
            "|      301|Alice|   10|       85|          78|          88|\n",
            "|      302|  Bob|   10|       92|          85|          79|\n",
            "|      304|David|    9|       88|          82|          85|\n",
            "|      305|  Eva|   10|       95|          89|          92|\n",
            "|      308|Henry|    9|       82|          79|          84|\n",
            "|      309|Irene|   10|       90|          88|          91|\n",
            "+---------+-----+-----+---------+------------+------------+\n",
            "\n",
            "DataFrame after adding TotalScore, renaming Class to Grade, and dropping EnglishScore:\n",
            "+---------+-------+-----+---------+------------+----------+\n",
            "|StudentID|   Name|Grade|MathScore|ScienceScore|TotalScore|\n",
            "+---------+-------+-----+---------+------------+----------+\n",
            "|      301|  Alice|   10|       85|          78|       251|\n",
            "|      302|    Bob|   10|       92|          85|       256|\n",
            "|      303|Charlie|    9|       70|          75|       225|\n",
            "|      304|  David|    9|       88|          82|       255|\n",
            "|      305|    Eva|   10|       95|          89|       276|\n",
            "|      306|  Frank|    9|       60|          65|       195|\n",
            "|      307|  Grace|   10|       78|          80|       234|\n",
            "|      308|  Henry|    9|       82|          79|       245|\n",
            "|      309|  Irene|   10|       90|          88|       269|\n",
            "|      310|   John|    9|       68|          72|       215|\n",
            "+---------+-------+-----+---------+------------+----------+\n",
            "\n",
            "Average Scores:\n",
            "Average Math Score: 80.80\n",
            "Average Science Score: 79.30\n",
            "Average Total Score: 242.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#out of syllabus\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, sum as _sum\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"HospitalPatientAnalysis\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (501, \"Alice\", 45, \"F\", \"Cardiology\", 3, 5000),\n",
        "    (502, \"Bob\", 62, \"M\", \"Orthopedics\", 6, 15000),\n",
        "    (503, \"Charlie\", 55, \"M\", \"Neurology\", 4, 12000),\n",
        "    (504, \"David\", 38, \"M\", \"Cardiology\", 2, 4000),\n",
        "    (505, \"Eva\", 70, \"F\", \"Orthopedics\", 7, 20000),\n",
        "    (506, \"Frank\", 50, \"M\", \"Neurology\", 5, 11000),\n",
        "    (507, \"Grace\", 65, \"F\", \"Cardiology\", 6, 18000),\n",
        "    (508, \"Henry\", 42, \"M\", \"Orthopedics\", 3, 8000),\n",
        "    (509, \"Irene\", 58, \"F\", \"Neurology\", 4, 13000),\n",
        "    (510, \"John\", 35, \"M\", \"Cardiology\", 2, 300)\n",
        "]\n",
        "\n",
        "# Define schema columns\n",
        "columns = [\"PatientID\", \"Name\", \"Age\", \"Gender\", \"Department\", \"VisitCount\", \"BillAmount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# 3. Filter patients with Age > 50 or VisitCount > 5\n",
        "filtered_df = df.filter((col(\"Age\") > 50) | (col(\"VisitCount\") > 5))\n",
        "\n",
        "# 4. Add new column Discount: 10% of BillAmount if Age > 60 else 0\n",
        "df = df.withColumn(\n",
        "    \"Discount\",\n",
        "    when(col(\"Age\") > 60, col(\"BillAmount\") * 0.10).otherwise(0)\n",
        ")\n",
        "\n",
        "# 5. Rename VisitCount to NumberOfVisits\n",
        "df = df.withColumnRenamed(\"VisitCount\", \"NumberOfVisits\")\n",
        "\n",
        "# 6. Drop Department column\n",
        "df = df.drop(\"Department\")\n",
        "\n",
        "# 7. Compute total bill amount per Gender\n",
        "total_bill_per_gender = df.groupBy(\"Gender\").agg(_sum(\"BillAmount\").alias(\"TotalBillAmount\"))\n",
        "\n",
        "# 8. Sort patients by BillAmount descending\n",
        "sorted_df = df.orderBy(col(\"BillAmount\").desc())\n",
        "\n",
        "# Show results\n",
        "print(\"Filtered patients (Age > 50 or VisitCount > 5):\")\n",
        "filtered_df.show()\n",
        "\n",
        "print(\"DataFrame after adding Discount, renaming, and dropping Department:\")\n",
        "df.show()\n",
        "\n",
        "print(\"Total Bill Amount per Gender:\")\n",
        "total_bill_per_gender.show()\n",
        "\n",
        "print(\"Patients sorted by BillAmount descending:\")\n",
        "sorted_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhgdD7n1i0MD",
        "outputId": "321ed452-40a8-402e-f4c5-9b951cc456b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered patients (Age > 50 or VisitCount > 5):\n",
            "+---------+-------+---+------+-----------+----------+----------+\n",
            "|PatientID|   Name|Age|Gender| Department|VisitCount|BillAmount|\n",
            "+---------+-------+---+------+-----------+----------+----------+\n",
            "|      502|    Bob| 62|     M|Orthopedics|         6|     15000|\n",
            "|      503|Charlie| 55|     M|  Neurology|         4|     12000|\n",
            "|      505|    Eva| 70|     F|Orthopedics|         7|     20000|\n",
            "|      507|  Grace| 65|     F| Cardiology|         6|     18000|\n",
            "|      509|  Irene| 58|     F|  Neurology|         4|     13000|\n",
            "+---------+-------+---+------+-----------+----------+----------+\n",
            "\n",
            "DataFrame after adding Discount, renaming, and dropping Department:\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "|PatientID|   Name|Age|Gender|NumberOfVisits|BillAmount|Discount|\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "|      501|  Alice| 45|     F|             3|      5000|     0.0|\n",
            "|      502|    Bob| 62|     M|             6|     15000|  1500.0|\n",
            "|      503|Charlie| 55|     M|             4|     12000|     0.0|\n",
            "|      504|  David| 38|     M|             2|      4000|     0.0|\n",
            "|      505|    Eva| 70|     F|             7|     20000|  2000.0|\n",
            "|      506|  Frank| 50|     M|             5|     11000|     0.0|\n",
            "|      507|  Grace| 65|     F|             6|     18000|  1800.0|\n",
            "|      508|  Henry| 42|     M|             3|      8000|     0.0|\n",
            "|      509|  Irene| 58|     F|             4|     13000|     0.0|\n",
            "|      510|   John| 35|     M|             2|       300|     0.0|\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "\n",
            "Total Bill Amount per Gender:\n",
            "+------+---------------+\n",
            "|Gender|TotalBillAmount|\n",
            "+------+---------------+\n",
            "|     F|          56000|\n",
            "|     M|          50300|\n",
            "+------+---------------+\n",
            "\n",
            "Patients sorted by BillAmount descending:\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "|PatientID|   Name|Age|Gender|NumberOfVisits|BillAmount|Discount|\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "|      505|    Eva| 70|     F|             7|     20000|  2000.0|\n",
            "|      507|  Grace| 65|     F|             6|     18000|  1800.0|\n",
            "|      502|    Bob| 62|     M|             6|     15000|  1500.0|\n",
            "|      509|  Irene| 58|     F|             4|     13000|     0.0|\n",
            "|      503|Charlie| 55|     M|             4|     12000|     0.0|\n",
            "|      506|  Frank| 50|     M|             5|     11000|     0.0|\n",
            "|      508|  Henry| 42|     M|             3|      8000|     0.0|\n",
            "|      501|  Alice| 45|     F|             3|      5000|     0.0|\n",
            "|      504|  David| 38|     M|             2|      4000|     0.0|\n",
            "|      510|   John| 35|     M|             2|       300|     0.0|\n",
            "+---------+-------+---+------+--------------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}